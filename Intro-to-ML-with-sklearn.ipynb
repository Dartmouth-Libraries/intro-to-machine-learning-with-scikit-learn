{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"120\" height=\"120\" style='padding: 0px 30px;' src=\"images/RR-logo.png\"/>\n",
    "\n",
    "# Introduction to Machine Learning with scikit-learn\n",
    "This notebook will introduce the popular (and free!) machine learning toolkit [``scikit-learn``](https://scikit-learn.org/) written in Python. ``scikit-learn`` offers algorithms for supervised and unsupervised learning, dimensionality reduction, model selection and evaluation, and even some techniques for the visualization of results. It is designed to integrate nicely with other popular toolkits like [matplotlib](https://matplotlib.org/) and [plotly](https://plotly.com/) (for visualizations), [pandas](https://pandas.pydata.org/) (for tabular data analysis and manipulation), and [NumPy](https://numpy.org/) and [SciPy](https://scipy.org/) (for scientific computing). We will see some of this integration later!\n",
    "\n",
    "\n",
    "In this exercise, we will walk through a typical processing pipeline to train and evaluate a simple classifier. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Click into a cell and hit \"Shift+Enter\" to execute it!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Let's apply some Dartmouth-style colors to our plots \"\"\"\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "dartmouth_colors = [\"#00693E\", \"#12312B\", \"#C3DD88\", \"#6EAA8D\", \"#797979\", \"#EBF3EF\"]\n",
    "\n",
    "mpl.rcParams.update({\n",
    "                        'figure.facecolor': \"#EBF3EF\",\n",
    "                        'figure.figsize': [7.50, 3.50],\n",
    "                        'axes.prop_cycle': mpl.cycler(color=dartmouth_colors),\n",
    "                        'axes.facecolor': \"#FFFFFF\",\n",
    "                        'axes.labelcolor': '#12312B',\n",
    "                        'text.color': '#12312B'\n",
    "                    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a dataset\n",
    "\n",
    "In this notebook, we will work with the famous *Iris* flower dataset. It is a multivariate dataset consisting of 50 samples from each of three species of *Iris*. Each sample is described in terms of four features: the length and width of its sepals and its petals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\" >\n",
    "table {\n",
    "    border-style: hidden;\n",
    "    border-collapse: collapse;\n",
    "    text-align: center;\n",
    "    border-top: 3px solid;\n",
    "    border-bottom: 3px solid;\n",
    "}\n",
    "\n",
    "tr, td, th {\n",
    "    border-bottom: none !important;\n",
    "    border-left: none !important;\n",
    "    border-right: none !important;\n",
    "}\n",
    "\n",
    "</style>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Iris setosa</th>\n",
    "    <th>Iris versicolor</th>\n",
    "    <th>Iris virginica</th>    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img align=\"center\" width=\"200\" height=\"200\" src=\"images/iris_setosa.jpg\"/></td>\n",
    "    <td><img align=\"center\" width=\"200\" height=\"200\" src=\"images/iris_versicolor.jpg\"/></td>\n",
    "    <td><img align=\"center\" width=\"200\" height=\"200\" src=\"images/iris_virginica.jpg\"/></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><a href=\"https://commons.wikimedia.org/wiki/File:Iris_setosa_2.jpg\" target=\"_blank\">\"Iris setosa\"</a> by <a href=\"https://commons.wikimedia.org/wiki/User:Kulmalukko\" target=\"_blank\">Tiia Monto</a><br> is licensed under <a href=\"http://creativecommons.org/licenses/by-sa/4.0\" target=\"_blank\">CC BY-SA 4.0</a></td>\n",
    "    <td><a href=\"https://commons.wikimedia.org/wiki/File:Blue_Flag_Iris_(15246206044).jpg\" target=\"_blank\">\"Blue Flag Iris\"</a> by <a href=\"https://www.flickr.com/people/49208525@N08\" target=\"_blank\">USFWSmidwest</a><br> is licensed under <a href=\"http://creativecommons.org/licenses/by/2.0\" target=\"_blank\">CC BY 2.0</a></td>\n",
    "    <td><a href=\"https://commons.wikimedia.org/wiki/File:Iris_virginica_L_JdP_2013-05-28_n01.jpg\" target=\"_blank\">\"Virginia Iris\"</a> by <a>Marie-Lan Nguyen</a><br> is licensed under <a href=\"http://creativecommons.org/licenses/by/2.5\" target=\"_blank\">CC BY 2.5</a></td>    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Class 0</td>\n",
    "    <td>Class 1</td>\n",
    "    <td>Class 2</td>    \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was originally collected by the American botanist Edgar Anderson [[1]](#anderson_1936). It is so well known today because of its use by the British statistician and biologist Ronald Fisher, who used it to showcase his method of Linear Discriminant Analysis [[2]](#fisher_1936). Because of the dataset's simplicity and interesting structure, it lends itself well to demonstrations of all sorts of algorithms and is therefore still popular today among machine learning educators.\n",
    "\n",
    "It is in fact so popular that it is shipped with ``scikit-learn`` and can be easily loaded into our workspace using the function [``load_iris()``](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) from the submodule ``sklearn.datasets``. \n",
    "\n",
    "Because it offers loads of convenient methods to manipulate and visualize our tabular data, we want to load the dataset as a [``pandas.DataFrame``](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) object. We can do this by setting the parameter ``as_frame`` to ``True``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "dataset = load_iris(as_frame=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we load the dataset in this, way we get a special, ``scikit-learn``-specific data structure called a [``Bunch``](https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html#sklearn.utils.Bunch). A ``Bunch`` is very much like a standard Python [dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries): It stores pairs of *keys* and *values*. We can obtain the *values* by passing the *key*. We can get a list of all the keys in the ``Bunch`` by calling its ``keys()`` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to explore the values corresponding to the other keys, but in this notebook we will simply grab the ``DataFrame`` that is stored under ``frame`` and the names of the features (``'feature_names'``) and species (``'target_names'``):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = dataset['frame']\n",
    "target_names = dataset['target_names']\n",
    "feature_names = dataset['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick sidenote:** A `DataFrame` is an object defined in the package `pandas`! `scikit-learn` uses it, because it is a great way to represent the tabular data. This is an example of the integration with other packages mentioned above. Why re-invent the wheel if you can build upon the great work of others? After all, we are basically doing the same thing when we use `scikit-learn` in our own code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some quick visualization to see how many samples of each species we have in our dataset (the *class distribution*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target.value_counts().plot(kind='bar', rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we did not call any `scikit-learn` code here, but `DataFrame` methods, which are implemented in `pandas`! But we don't really need to worry about that, because everything is so neatly integrated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we have the same number of samples from each species (or class). That means our dataset is *balanced*. This is a good thing because in an *imbalanced* dataset, machine learning models tend to be better at identifying the majority class than at identifying the minority class (i.e., a high bias) and we need to apply special techniques to counter this behavior (e.g. over- or undersampling, or special scoring metrics). Our balanced Iris dataset does not need any special attention in that regard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us take a look a the features and if we can spot any patterns in them!\n",
    "\n",
    "One way to do this is to plot all the two-dimensional scatter plots:\n",
    "- *sepal length* versus *sepal width*\n",
    "- *petal length* versus *petal width*\n",
    "- *sepal length* versus *petal length*\n",
    "- ...\n",
    "\n",
    "That is a lot of pairwise plots. Luckily, we can use a function from the package [``seaborn``](https://seaborn.pydata.org/index.html) called [``pairplot()``](https://seaborn.pydata.org/generated/seaborn.pairplot.html) that conveniently does all the work for us!\n",
    "\n",
    "`seaborn` is a library for statistical data visualization that plays very nicely with `DataFrame` objects. So let's import and use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(data=iris, hue='target', palette=sns.color_palette(dartmouth_colors)[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in some plots, dots of the same color are neatly clustered and the clusters overlap very little. Others are a bit more messy. Can you guess, which species (or *class*) may be harder to distinguish from the other two?\n",
    "\n",
    "We also suspect some strong correlations between features. Let's investigate by calculating the pair-wise correlation using the ``DataFrame`` method [``corr()``](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris[feature_names].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we have quite strongly correlated features in this dataset! \n",
    "\n",
    "In machine learning, we generally prefer uncorrelated features because correlated features add only very little information that can help to differentiate between the classes.\n",
    "\n",
    "We should keep the strong correlations in mind for later. As a general rule, a simple model using few features is preferred over a complex model using many features. So maybe we don't even need to use all features in our model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common issue when dealing with a multivariate dataset is that the different features can be on very different scales. That means that when we compute distances in the feature space, features with generally high values will contribute disproportionately to the overall distance.\n",
    "\n",
    "Before we process our data further, it is therefore usually a good idea to normalize or standardize our data. A common standardization method is to calculate the *$z$-score*. We subtract the mean $\\mu$ and divide by the standard deviation $\\sigma$ from each feature value $x$: \n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "A $z$-score of 1 means *1 standard deviation from the mean* on the original scale. This makes distance calculations much less biased and therefore more meaningful!\n",
    "\n",
    "Conveniently, `scikit-learn` offers a [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) object to do this very easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = iris[feature_names]\n",
    "y = iris['target']\n",
    "\n",
    "scaler = StandardScaler()       # Instantiate the scaler\n",
    "scaler.fit(X)                   # Fit the scaler on the data and calculate its parameters (mean and variance)\n",
    "print(f'The feature means are {scaler.mean_} and their variances are {scaler.var_}')\n",
    "\n",
    "X_scaled = scaler.transform(X)  # Transform the data (using the previously calculated mean and variance)\n",
    "print(f'After the transformation, the feature means are {X_scaled.mean(axis=0)} and their variances {X_scaled.var(axis=0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that using the `StandardScaler` object is a three step sequence:\n",
    "1. Instantiate the object\n",
    "2. Fit the object's parameters on the data\n",
    "3. Transform the data using the fitted object\n",
    "\n",
    "This basic pattern is the core workflow in `scikit-learn` and we will see it over and over again in the rest of this notebooks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "X_pc = pca.transform(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(X_pc, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing an estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=8)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train_pc = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "knn.fit(X_train_pc, y_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_pc = pca.transform(X_test_scaled)\n",
    "\n",
    "y_pred = knn.predict(X_test_pc)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
    "    'p': [1, 2, 3]    \n",
    "}\n",
    "\n",
    "search = GridSearchCV(KNeighborsClassifier(), param_grid, verbose=1, n_jobs=-1)\n",
    "search.fit(X_train_pc, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, search.predict(X_test_pc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "steps = [\n",
    "    ('scaling', StandardScaler()),\n",
    "    ('pca', PCA()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'scaling__with_mean': [True, False],\n",
    "    'scaling__with_std': [True, False],\n",
    "    'pca__n_components': [1, 2, 3, 4],\n",
    "    'knn__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'knn__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'knn__leaf_size': [10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
    "    'knn__p': [1, 2, 3]    \n",
    "}\n",
    "\n",
    "find_best = GridSearchCV(pipeline, param_grid, verbose=1, n_jobs=-1)\n",
    "\n",
    "find_best.fit(X_train, y_train)\n",
    "find_best.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results = pd.DataFrame(find_best.cv_results_)\n",
    "results.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, find_best.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] <a id=anderson_1936></a>Edgar Anderson (1936). \"The species problem in Iris\". *Annals of the Missouri Botanical Garden*. 23 (3): 457–509. [doi:10.2307/2394164](https://doi.org/10.2307%2F2394164). JSTOR [2394164](https://www.jstor.org/stable/2394164).\n",
    "\n",
    "[2] <a id=fisher_1936></a>R. A. Fisher (1936). \"The use of multiple measurements in taxonomic problems\". *Annals of Eugenics*. 7 (2): 179–188. [doi:10.1111/j.1469-1809.1936.tb02137.x](https://doi.org/10.1111%2Fj.1469-1809.1936.tb02137.x). hdl:[2440/15227](https://hdl.handle.net/2440%2F15227). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
